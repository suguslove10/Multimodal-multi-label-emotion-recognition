# ğŸ­ AI-Powered Multimodal Emotion Recognition

Modern full-stack emotion recognition application using Next.js, FastAPI, and state-of-the-art AI models.

![Tech Stack](https://img.shields.io/badge/Next.js-16-black)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104-green)
![Python](https://img.shields.io/badge/Python-3.10--3.13-blue)
![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue)

## âœ¨ Features

- ğŸ¨ **Modern UI** - Beautiful gradient design with Tailwind CSS v4
- ğŸ“ **Text Analysis** - Emotion detection from written text
- ğŸ“¸ **Image Upload** - Facial emotion analysis from photos
- ğŸ¤ **Voice Analysis** - Audio recording and voice tone emotion detection
- ğŸ¤– **Machine Learning** - Decision Tree and KNN predictions
- ğŸ“Š **Visualizations** - Interactive confidence charts
- ğŸ’¾ **History Tracking** - Save and review past analyses

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10-3.13 (NOT 3.14)
- Node.js 18+ and npm
- Microphone access
- 2-3 GB disk space for AI models

### Installation & Run

**1. Start Backend (Terminal 1):**
```bash
# Mac/Linux
./start-backend.sh

# Windows
start-backend.bat
```

**2. Start Frontend (Terminal 2):**
```bash
# Mac/Linux
./start-frontend.sh

# Windows
start-frontend.bat
```

**3. Open Browser:**
- **Frontend:** http://localhost:3000
- **Backend API:** http://localhost:8000
- **API Docs:** http://localhost:8000/docs

## ğŸ“– Usage

1. **Text:** Type how you're feeling in the text box
2. **Image:** Click "Upload" and select a photo (or use "Camera" if available)
3. **Audio:** Click "Start Recording", speak for 5-10 seconds, then click "Stop"
4. **Analyze:** Click "Analyze Emotions" button
5. **Results:** View emotion breakdown, confidence chart, and ML predictions

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js UI    â”‚ â—„â”€â”€â”€â”€â”€â–º â”‚  FastAPI Server â”‚
â”‚  (Port 3000)    â”‚  HTTP   â”‚   (Port 8000)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚   AI Models     â”‚
                            â”‚  - Text (RoBERTa)â”‚
                            â”‚  - Image (ViT)  â”‚
                            â”‚  - Audio (Wav2Vec2)â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tech Stack

### Frontend
- Next.js 16 (App Router, Turbopack)
- TypeScript
- Tailwind CSS v4
- Lucide React Icons

### Backend
- FastAPI
- Hugging Face Transformers
- PyTorch
- Scikit-learn (Decision Tree, KNN)
- Pandas & Matplotlib

### AI Models
- **Text:** `j-hartmann/emotion-english-distilroberta-base`
- **Image:** `trpakov/vit-face-expression`
- **Audio:** `ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition`

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI server & AI logic
â”‚   â””â”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx        # Main UI component
â”‚   â”‚   â”œâ”€â”€ layout.tsx      # Root layout
â”‚   â”‚   â””â”€â”€ globals.css     # Global styles
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ start-backend.sh         # Backend startup (Mac/Linux)
â”œâ”€â”€ start-frontend.sh        # Frontend startup (Mac/Linux)
â”œâ”€â”€ start-backend.bat        # Backend startup (Windows)
â”œâ”€â”€ start-frontend.bat       # Frontend startup (Windows)
â””â”€â”€ README.md
```

## ğŸ› Troubleshooting

**Backend won't start:**
- Check Python version: `python3 --version` (must be 3.10-3.13)
- Recreate venv: `rm -rf venv && python3.13 -m venv venv`
- First run downloads ~1-2GB of AI models (takes 1-2 minutes)

**Frontend won't start:**
- Check Node.js: `node --version` (must be 18+)
- Reinstall: `cd frontend && rm -rf node_modules && npm install`

**Analysis fails:**
- Ensure backend shows "âœ… Models loaded successfully"
- Check backend is running on port 8000
- Check browser console (F12) for errors

**Audio not working:**
- Grant microphone permissions in browser
- Speak clearly for 5-10 seconds
- Use Chrome or Edge for best compatibility

## ğŸ“Š API Endpoints

- `GET /` - Health check
- `GET /health` - Models status
- `POST /analyze` - Analyze emotions (multipart/form-data)
  - `text`: string
  - `image`: file
  - `audio`: file
- `GET /history?limit=10` - Get analysis history

## ğŸš€ Deployment

### Backend (Railway/Render/AWS)
```bash
pip install -r backend/requirements.txt
uvicorn backend.main:app --host 0.0.0.0 --port $PORT
```

### Frontend (Vercel - Recommended)
```bash
cd frontend
npm run build
```
Set environment variable: `NEXT_PUBLIC_API_URL=https://your-backend-url.com`

## ğŸ“ License

MIT License

## ğŸ™ Acknowledgments

- Hugging Face for pre-trained models
- Next.js and FastAPI teams
- Open source community

---

**Made with â¤ï¸ using Next.js, FastAPI, and AI**
